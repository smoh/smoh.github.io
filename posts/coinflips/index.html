<!DOCTYPE html>
<html><head>
    
    <link rel="stylesheet" href="https://cdn.simplecss.org/simple.min.css">
    
    <link rel="stylesheet" href="https://smoh.space/css/custom.css">
</head><body><header>
    <nav>

        <a href="/">Home</a>
        <a href="/posts">Posts</a>

        <h1>Biased coins and truncated data</h1>
        
        <p><small>Posted on May 24, 2020</small></p>
        
    </nav>
</header><main>





<p>I was inspired by a recent paper by <a href="https://arxiv.org/abs/2005.08983">Boubert &amp; Everall 2020</a> to consider
the inference of the probability of a biased coin.</p>
<p>The problem is the following: given a table of $N$ coin flip experiments where you have, for each experiment,
the number of times it was flipped and the number of times it showed heads, we want to infer
the probability that this coin shows head.</p>
<p>Let&rsquo;s call</p>
<ul>
<li>$i = 1,\ldots,N$: index of the experiments</li>
<li>$n_i$: the number of times the coin was flipped in the $i$-th experiment</li>
<li>$k_i$: the number of times it showed head in the $i$-th experiment</li>
<li>$\theta$: the probability that this coin shows head (what we want to know).</li>
</ul>
<p>The probability that the coin shows head $k$ times is described by a Binomial distribution, $B(n_i,\theta)$:</p>
<p>$$k_i \sim B(n_i,\theta)$$</p>
<p>For the prior of $\theta$, let&rsquo;s assume uniform between 0 and 1.</p>
<p>The model is quite simple in stan.</p>
<details>
<summary># import libraries</summary>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> os<span style="color:#f92672">,</span> pickle
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> pystan
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> scipy <span style="color:#f92672">import</span> stats
</span></span><span style="display:flex;"><span><span style="color:#f92672">%</span>matplotlib inline 
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#66d9ef">as</span> plt
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> seaborn <span style="color:#66d9ef">as</span> sns
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>style<span style="color:#f92672">.</span>use(<span style="color:#e6db74">&#39;smoh&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>rc(<span style="color:#e6db74">&#39;figure&#39;</span>, facecolor<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;w&#39;</span>)
</span></span></code></pre></div></details>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>cache_pickle <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;model.pkl&#39;</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">if</span> <span style="color:#f92672">not</span> os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>exists(cache_pickle):
</span></span><span style="display:flex;"><span>    model <span style="color:#f92672">=</span> pystan<span style="color:#f92672">.</span>StanModel(model_code<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;&#39;&#39;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">data {
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    // number of coins
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    int&lt;lower=0&gt; N;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    // number of flips
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    int n[N];
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    // number of heads
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    int k[N];
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">}
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">parameters {
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    real&lt;lower=0,upper=1&gt; theta;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">}
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">model {
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    for (i in 1:N) {
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        k[i] ~ binomial(n[i], theta);
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    }
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">}
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">&#39;&#39;&#39;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">with</span> open(cache_pickle, <span style="color:#e6db74">&#39;wb&#39;</span>) <span style="color:#66d9ef">as</span> f:
</span></span><span style="display:flex;"><span>        pickle<span style="color:#f92672">.</span>dump(model, f)
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">&#34;reading model from disk&#34;</span>)
</span></span><span style="display:flex;"><span>    model <span style="color:#f92672">=</span> pickle<span style="color:#f92672">.</span>load(open(cache_pickle, <span style="color:#e6db74">&#39;rb&#39;</span>))
</span></span></code></pre></div><pre><code>reading model from disk
</code></pre>
<p>We can simulate data with <code>scipy.stats.binom</code>.
If we have no truncation on the data, different experiments do not matter.
If the coin was flipped 20 times in a hundred experiment, it is the same as one experiment of flipping it
2000 times.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>theta <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.63</span>
</span></span><span style="display:flex;"><span>N <span style="color:#f92672">=</span> <span style="color:#ae81ff">100</span>
</span></span><span style="display:flex;"><span>n_i <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>ones(N, dtype<span style="color:#f92672">=</span>int)<span style="color:#f92672">*</span><span style="color:#ae81ff">20</span>
</span></span><span style="display:flex;"><span>k_i <span style="color:#f92672">=</span> stats<span style="color:#f92672">.</span>binom<span style="color:#f92672">.</span>rvs(n<span style="color:#f92672">=</span>n_i, p<span style="color:#f92672">=</span>theta)
</span></span><span style="display:flex;"><span>data <span style="color:#f92672">=</span> dict(N<span style="color:#f92672">=</span>N, n<span style="color:#f92672">=</span>n_i, k<span style="color:#f92672">=</span>k_i)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># combine all experiment to one</span>
</span></span><span style="display:flex;"><span>data_one <span style="color:#f92672">=</span> dict(N<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, n<span style="color:#f92672">=</span>[n_i<span style="color:#f92672">.</span>sum()], k<span style="color:#f92672">=</span>[k_i<span style="color:#f92672">.</span>sum()])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>ropt <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>optimizing(data<span style="color:#f92672">=</span>data)
</span></span><span style="display:flex;"><span>ropt_one <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>optimizing(data<span style="color:#f92672">=</span>data_one)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;theta from </span><span style="color:#e6db74">{}</span><span style="color:#e6db74"> experiments of </span><span style="color:#e6db74">{}</span><span style="color:#e6db74"> flips = </span><span style="color:#e6db74">{:.3f}</span><span style="color:#e6db74">&#39;</span><span style="color:#f92672">.</span>format(N, n_i[<span style="color:#ae81ff">0</span>],ropt[<span style="color:#e6db74">&#39;theta&#39;</span>]))
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;theta from </span><span style="color:#e6db74">{}</span><span style="color:#e6db74"> experiments of </span><span style="color:#e6db74">{}</span><span style="color:#e6db74"> flips = </span><span style="color:#e6db74">{:.3f}</span><span style="color:#e6db74">&#39;</span><span style="color:#f92672">.</span>format(<span style="color:#ae81ff">1</span>, n_i<span style="color:#f92672">.</span>sum(),ropt[<span style="color:#e6db74">&#39;theta&#39;</span>]))
</span></span></code></pre></div><pre><code>theta from 100 experiments of 20 flips = 0.638
theta from 1 experiments of 2000 flips = 0.638
</code></pre>
<p>In fact, the uniform distribution is a special case of Beta distribution $\mathrm{Beta}(\alpha,,\beta)$ when
$\alpha=\beta=1$.
Because Beta distributions are conjugate priors of Binomial distribution,
the posterior distribution is also a Beta distribution with
$\mathrm{Beta}(\alpha+k,,\beta+n-k)$ where $n=\Sigma n_i$ and $k=\Sigma k_i$.
The posterior mean is $\frac{\alpha+k}{\alpha+\beta+n}$, which approaches $\frac{k}{n}$ as $n \to \infty$, as it should intuitively.</p>
<p>We can check that stan posterior sample matches this expectation.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>r1 <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>sampling(data<span style="color:#f92672">=</span>data)
</span></span><span style="display:flex;"><span>r2 <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>sampling(data<span style="color:#f92672">=</span>data_one)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>fig, ax <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplots(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">4</span>,<span style="color:#ae81ff">3</span>))
</span></span><span style="display:flex;"><span>sns<span style="color:#f92672">.</span>kdeplot(r1[<span style="color:#e6db74">&#39;theta&#39;</span>], label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;$(N, n_i)=(100,20)$&#39;</span>,ax<span style="color:#f92672">=</span>ax)
</span></span><span style="display:flex;"><span>sns<span style="color:#f92672">.</span>kdeplot(r2[<span style="color:#e6db74">&#39;theta&#39;</span>], label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;$(N, n_i)=(1,2000)$&#39;</span>, ax<span style="color:#f92672">=</span>ax)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>x <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>linspace(<span style="color:#f92672">*</span>ax<span style="color:#f92672">.</span>get_xlim(), <span style="color:#ae81ff">64</span>)
</span></span><span style="display:flex;"><span>pdfx<span style="color:#f92672">=</span> stats<span style="color:#f92672">.</span>beta(<span style="color:#ae81ff">1</span><span style="color:#f92672">+</span>k_i<span style="color:#f92672">.</span>sum(), <span style="color:#ae81ff">1</span><span style="color:#f92672">+</span>n_i<span style="color:#f92672">.</span>sum()<span style="color:#f92672">-</span>k_i<span style="color:#f92672">.</span>sum())<span style="color:#f92672">.</span>pdf(x)
</span></span><span style="display:flex;"><span>ax<span style="color:#f92672">.</span>plot(x, pdfx, <span style="color:#e6db74">&#39;k-&#39;</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">r</span><span style="color:#e6db74">&#39;$\mathrm</span><span style="color:#e6db74">{Beta}</span><span style="color:#e6db74">(\alpha+k,\,\beta+n-k)$&#39;</span>);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>ax<span style="color:#f92672">.</span>axvline(theta, lw<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>ax<span style="color:#f92672">.</span>legend(loc<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;upper right&#39;</span>, fontsize<span style="color:#f92672">=</span><span style="color:#ae81ff">12</span>);
</span></span><span style="display:flex;"><span>ax<span style="color:#f92672">.</span>set(ylim<span style="color:#f92672">=</span>(<span style="color:#ae81ff">0</span>,<span style="color:#ae81ff">70</span>), xlabel<span style="color:#f92672">=</span><span style="color:#e6db74">r</span><span style="color:#e6db74">&#39;$\theta$&#39;</span>, ylabel<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;pdf&#39;</span>);
</span></span></code></pre></div><p><img src="/files/coinflips/output_8_0.png" alt="png"></p>
<p>Now, what if it turns out that the experiment is not reported at all when the number of times it showed head is less than $k_t$ times?
As explained in <a href="https://arxiv.org/abs/2005.08983">Boubert &amp; Everall 2020</a>, we need to
adjust the likelihood function by dividing it by
$$\int_{k_t}^n p(k |n,\theta)$$
(so that it will integrate to 1 from $k_t$ not $1$).</p>
<p>Stan supports truncated data for 1D distributions[1]
so the modification is simple:</p>
<p>[1] <a href="https://mc-stan.org/docs/2_20/stan-users-guide/truncated-data-section.html">https://mc-stan.org/docs/2_20/stan-users-guide/truncated-data-section.html</a></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>cache_pickle <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;modelth.pkl&#39;</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">if</span> <span style="color:#f92672">not</span> os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>exists(cache_pickle):
</span></span><span style="display:flex;"><span>    modelth <span style="color:#f92672">=</span> pystan<span style="color:#f92672">.</span>StanModel(model_code<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;&#39;&#39;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">data {
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    // number of coins
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    int&lt;lower=0&gt; N;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    // number of flips
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    int n[N];
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    // number of heads
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    int k[N];
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    // truncation threshold
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    int kthresh;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">}
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">parameters {
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    real&lt;lower=0,upper=1&gt; theta;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">}
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">model {
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    for (i in 1:N) {
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        k[i] ~ binomial(n[i], theta) T[kthresh,];
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    }
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">}
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#39;&#39;&#39;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">with</span> open(cache_pickle, <span style="color:#e6db74">&#39;wb&#39;</span>) <span style="color:#66d9ef">as</span> f:
</span></span><span style="display:flex;"><span>        pickle<span style="color:#f92672">.</span>dump(modelth, f)
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">&#34;reading model from disk&#34;</span>)
</span></span><span style="display:flex;"><span>    modelth <span style="color:#f92672">=</span> pickle<span style="color:#f92672">.</span>load(open(cache_pickle, <span style="color:#e6db74">&#39;rb&#39;</span>))
</span></span></code></pre></div><pre><code>reading model from disk
</code></pre>
<p>Now, because a record of an experiment might be missing,
it is no longer valid to treat the sum of experiments as one &lsquo;master&rsquo; experiment.</p>
<p>What do we expect for our inference of $\theta$ if we use the truncated data
yet use the old (wrong) model that does not account for missing data?
The data will be biased towards high $k_i$ and this will bias $\theta$ high as well.</p>
<p>Let&rsquo;s simulate this truncated data as well and see if this is the case.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>theta <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.63</span>
</span></span><span style="display:flex;"><span>N <span style="color:#f92672">=</span> <span style="color:#ae81ff">100</span>
</span></span><span style="display:flex;"><span>n_i <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>ones(N, dtype<span style="color:#f92672">=</span>int)<span style="color:#f92672">*</span><span style="color:#ae81ff">10</span>
</span></span><span style="display:flex;"><span>k_i <span style="color:#f92672">=</span> stats<span style="color:#f92672">.</span>binom<span style="color:#f92672">.</span>rvs(n<span style="color:#f92672">=</span>n_i, p<span style="color:#f92672">=</span>theta)
</span></span><span style="display:flex;"><span>k_t <span style="color:#f92672">=</span> <span style="color:#ae81ff">5</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># repeat until i have N experiments replacing truncated</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">while</span> (k_i<span style="color:#f92672">&lt;</span>k_t)<span style="color:#f92672">.</span>sum()<span style="color:#f92672">&gt;</span><span style="color:#ae81ff">0</span>:
</span></span><span style="display:flex;"><span>    bad <span style="color:#f92672">=</span> k_i <span style="color:#f92672">&lt;</span> k_t
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">&#39;replacing </span><span style="color:#e6db74">{}</span><span style="color:#e6db74">&#39;</span><span style="color:#f92672">.</span>format(bad<span style="color:#f92672">.</span>sum()))
</span></span><span style="display:flex;"><span>    k_i_new <span style="color:#f92672">=</span> stats<span style="color:#f92672">.</span>binom<span style="color:#f92672">.</span>rvs(n<span style="color:#f92672">=</span>n_i[bad], p<span style="color:#f92672">=</span>theta)
</span></span><span style="display:flex;"><span>    k_i[bad] <span style="color:#f92672">=</span> k_i_new
</span></span><span style="display:flex;"><span>data <span style="color:#f92672">=</span> dict(N<span style="color:#f92672">=</span>N, n<span style="color:#f92672">=</span>n_i, k<span style="color:#f92672">=</span>k_i,)
</span></span></code></pre></div><pre><code>replacing 20
replacing 2
</code></pre>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>r1 <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>sampling(data<span style="color:#f92672">=</span>data)
</span></span><span style="display:flex;"><span>data[<span style="color:#e6db74">&#39;kthresh&#39;</span>] <span style="color:#f92672">=</span> k_t
</span></span><span style="display:flex;"><span>r2 <span style="color:#f92672">=</span> modelth<span style="color:#f92672">.</span>sampling(data<span style="color:#f92672">=</span>data)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>fig, ax <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplots(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">4</span>,<span style="color:#ae81ff">3</span>))
</span></span><span style="display:flex;"><span>sns<span style="color:#f92672">.</span>kdeplot(r1[<span style="color:#e6db74">&#39;theta&#39;</span>], label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;old model&#39;</span>,ax<span style="color:#f92672">=</span>ax)
</span></span><span style="display:flex;"><span>sns<span style="color:#f92672">.</span>kdeplot(r2[<span style="color:#e6db74">&#39;theta&#39;</span>], label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;truncated data model&#39;</span>, ax<span style="color:#f92672">=</span>ax)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>ax<span style="color:#f92672">.</span>axvline(theta, lw<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>ax<span style="color:#f92672">.</span>legend(loc<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;upper left&#39;</span>, fontsize<span style="color:#f92672">=</span><span style="color:#ae81ff">12</span>);
</span></span><span style="display:flex;"><span>ax<span style="color:#f92672">.</span>set(ylim<span style="color:#f92672">=</span>(<span style="color:#ae81ff">0</span>,<span style="color:#ae81ff">40</span>), xlabel<span style="color:#f92672">=</span><span style="color:#e6db74">r</span><span style="color:#e6db74">&#39;$\theta$&#39;</span>, ylabel<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;pdf&#39;</span>);
</span></span></code></pre></div><p><img src="/files/coinflips/output_14_0.png" alt="png"></p>
<p>Indeed, the old model that does not account for missing data infers
a higher $\theta$ than we should whereas the new model recovers the correct value.</p>
<p>This will be more prounounced as we repeat more experiments and the posterior
distribution gets sharper.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>rs_old <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>rs_new <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> N <span style="color:#f92672">in</span> [<span style="color:#ae81ff">100</span>, <span style="color:#ae81ff">1000</span>]:
</span></span><span style="display:flex;"><span>    n_i <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>ones(N, dtype<span style="color:#f92672">=</span>int)<span style="color:#f92672">*</span><span style="color:#ae81ff">10</span>
</span></span><span style="display:flex;"><span>    k_i <span style="color:#f92672">=</span> stats<span style="color:#f92672">.</span>binom<span style="color:#f92672">.</span>rvs(n<span style="color:#f92672">=</span>n_i, p<span style="color:#f92672">=</span>theta)
</span></span><span style="display:flex;"><span>    k_t <span style="color:#f92672">=</span> <span style="color:#ae81ff">5</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># repeat until i have N experiments replacing truncated</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">while</span> (k_i<span style="color:#f92672">&lt;</span>k_t)<span style="color:#f92672">.</span>sum()<span style="color:#f92672">&gt;</span><span style="color:#ae81ff">0</span>:
</span></span><span style="display:flex;"><span>        bad <span style="color:#f92672">=</span> k_i <span style="color:#f92672">&lt;</span> k_t
</span></span><span style="display:flex;"><span>        k_i_new <span style="color:#f92672">=</span> stats<span style="color:#f92672">.</span>binom<span style="color:#f92672">.</span>rvs(n<span style="color:#f92672">=</span>n_i[bad], p<span style="color:#f92672">=</span>theta)
</span></span><span style="display:flex;"><span>        k_i[bad] <span style="color:#f92672">=</span> k_i_new
</span></span><span style="display:flex;"><span>    data_tmp <span style="color:#f92672">=</span> dict(N<span style="color:#f92672">=</span>N, n<span style="color:#f92672">=</span>n_i, k<span style="color:#f92672">=</span>k_i,)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">%</span>time r1tmp <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>sampling(data<span style="color:#f92672">=</span>data_tmp)
</span></span><span style="display:flex;"><span>    data_tmp[<span style="color:#e6db74">&#39;kthresh&#39;</span>] <span style="color:#f92672">=</span> k_t
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">%</span>time r2tmp <span style="color:#f92672">=</span> modelth<span style="color:#f92672">.</span>sampling(data<span style="color:#f92672">=</span>data_tmp)
</span></span><span style="display:flex;"><span>    rs_old<span style="color:#f92672">.</span>append(r1tmp)
</span></span><span style="display:flex;"><span>    rs_new<span style="color:#f92672">.</span>append(r2tmp)
</span></span></code></pre></div><pre><code>CPU times: user 91.2 ms, sys: 447 ms, total: 538 ms
Wall time: 715 ms
CPU times: user 102 ms, sys: 425 ms, total: 526 ms
Wall time: 1.67 s
CPU times: user 95.7 ms, sys: 440 ms, total: 536 ms
Wall time: 1.17 s
CPU times: user 134 ms, sys: 466 ms, total: 600 ms
Wall time: 9.09 s
</code></pre>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>fig, ax <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplots(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">4</span>,<span style="color:#ae81ff">3</span>))
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> i, (rr1, rr2) <span style="color:#f92672">in</span> enumerate(zip(rs_old, rs_new)):
</span></span><span style="display:flex;"><span>    sns<span style="color:#f92672">.</span>kdeplot(rr1[<span style="color:#e6db74">&#39;theta&#39;</span>], c<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;C0&#39;</span>, lw<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span> <span style="color:#66d9ef">if</span> i<span style="color:#f92672">&gt;</span><span style="color:#ae81ff">0</span> <span style="color:#66d9ef">else</span> <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>    sns<span style="color:#f92672">.</span>kdeplot(rr2[<span style="color:#e6db74">&#39;theta&#39;</span>], ax<span style="color:#f92672">=</span>ax, c<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;C1&#39;</span>,lw<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span> <span style="color:#66d9ef">if</span> i<span style="color:#f92672">&gt;</span><span style="color:#ae81ff">0</span> <span style="color:#66d9ef">else</span> <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>ax<span style="color:#f92672">.</span>axvline(theta, lw<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>ax<span style="color:#f92672">.</span>set(xlabel<span style="color:#f92672">=</span><span style="color:#e6db74">r</span><span style="color:#e6db74">&#39;$\theta$&#39;</span>, ylabel<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;pdf&#39;</span>);
</span></span></code></pre></div><p><img src="/files/coinflips/output_17_0.png" alt="png"></p>
<p>The correct model converges to the true value while the wrong model will bias $\theta$ higher.</p>
<p>Note that if $\theta n_i$ (the expectation value of $k_i$ for $i$-th experiment) is much larger than
the threshold $k_t$, than the threshold is not going to filter out much data and
it would not matter if we use the corret model that accounts for the missing data or not.</p>
<p>What is acutally happening here?</p>
<p>The posterior distribution is</p>
<p>$$ p(\theta| {n_i,k_i}) \propto \prod _{i=1}^{N}  p (k_i |n_i,,\theta) p(\theta) $$
where
$$p (k_i| n_i, \theta) = \frac{1}{1 - p(k_i \lt 5 | n_i, \theta)} {n \choose k} \theta^k \theta^{n-k}$$</p>
<p>Assuming uniform prior, $p(\theta) = 1$ between 0 and 1, let&rsquo;s have a look at
what the normalization factor that applies the data truncation does.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>fig, (ax1,ax2) <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplots(<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">2</span>, figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">8</span>,<span style="color:#ae81ff">3</span>))
</span></span><span style="display:flex;"><span>thetas <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>linspace(<span style="color:#ae81ff">0.01</span>,<span style="color:#ae81ff">0.99</span>,<span style="color:#ae81ff">201</span>)
</span></span><span style="display:flex;"><span>pdfs<span style="color:#f92672">=</span>[]
</span></span><span style="display:flex;"><span>n_flip <span style="color:#f92672">=</span> <span style="color:#ae81ff">10</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> k <span style="color:#f92672">in</span> np<span style="color:#f92672">.</span>arange(k_t, <span style="color:#ae81ff">11</span>):
</span></span><span style="display:flex;"><span>    ax1<span style="color:#f92672">.</span>plot(thetas, stats<span style="color:#f92672">.</span>binom<span style="color:#f92672">.</span>pmf(k, n_flip, thetas), <span style="color:#e6db74">&#39;-&#39;</span>,lw<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>,
</span></span><span style="display:flex;"><span>             c<span style="color:#f92672">=</span>plt<span style="color:#f92672">.</span>cm<span style="color:#f92672">.</span>plasma((k<span style="color:#f92672">-</span>k_t)<span style="color:#f92672">/</span>(n_flip<span style="color:#f92672">-</span>k_t)));
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>I <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span><span style="color:#f92672">-</span>stats<span style="color:#f92672">.</span>binom<span style="color:#f92672">.</span>cdf(k_t<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span> , n_flip, thetas)
</span></span><span style="display:flex;"><span>ax1<span style="color:#f92672">.</span>plot(thetas, I, c<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;k&#39;</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">r</span><span style="color:#e6db74">&#39;$1 - p(k_i &lt; 5\,|\,n_i,\,\theta)$&#39;</span>)
</span></span><span style="display:flex;"><span>ax1<span style="color:#f92672">.</span>legend(loc<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;upper left&#39;</span>, fontsize<span style="color:#f92672">=</span><span style="color:#ae81ff">12</span>)
</span></span><span style="display:flex;"><span>ax1<span style="color:#f92672">.</span>set(ylim<span style="color:#f92672">=</span>(<span style="color:#66d9ef">None</span>, <span style="color:#ae81ff">1.2</span>))
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> k <span style="color:#f92672">in</span> np<span style="color:#f92672">.</span>arange(k_t, <span style="color:#ae81ff">11</span>):
</span></span><span style="display:flex;"><span>    ax2<span style="color:#f92672">.</span>plot(thetas, stats<span style="color:#f92672">.</span>binom<span style="color:#f92672">.</span>pmf(k, <span style="color:#ae81ff">10</span>, thetas)<span style="color:#f92672">/</span>I, <span style="color:#e6db74">&#39;-&#39;</span>, lw<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>,
</span></span><span style="display:flex;"><span>             c<span style="color:#f92672">=</span>plt<span style="color:#f92672">.</span>cm<span style="color:#f92672">.</span>plasma((k<span style="color:#f92672">-</span>k_t)<span style="color:#f92672">/</span>(n_flip<span style="color:#f92672">-</span>k_t)), label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;k=</span><span style="color:#e6db74">{}</span><span style="color:#e6db74">&#39;</span><span style="color:#f92672">.</span>format(k));
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>ax2<span style="color:#f92672">.</span>legend(loc<span style="color:#f92672">=</span>(<span style="color:#ae81ff">0.2</span>,<span style="color:#ae81ff">0.6</span>), fontsize<span style="color:#f92672">=</span><span style="color:#ae81ff">12</span>, ncol<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>ax2<span style="color:#f92672">.</span>set(ylim<span style="color:#f92672">=</span>(<span style="color:#66d9ef">None</span>, <span style="color:#ae81ff">1.6</span>)) 
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> cax <span style="color:#f92672">in</span> (ax1, ax2):
</span></span><span style="display:flex;"><span>    cax<span style="color:#f92672">.</span>set(xlabel<span style="color:#f92672">=</span><span style="color:#e6db74">r</span><span style="color:#e6db74">&#39;$\theta$&#39;</span>)
</span></span></code></pre></div><p><img src="/files/coinflips/output_20_0.png" alt="png"></p>
<p>For each possible $k_i \ge k_t = 5$ upto $n_i=10$,
the plot above shows the usual Binomial probability mass function (pmf) on the left.
The normalization factor is the reciprocal of the black line, which shows $1 - p(k_i \lt 5 | n_i, \theta)$.
After the correction is applied, each pmf on the left of the same color is changed to that on the right.</p>
<p>In essense, the correction compensates for the fact that
the observation of $k_i$ heads may be statistical fluctuation above the mean at lower $\theta$.
We see that if $\theta$ is large enough to put typical number of $k_i$ above the threshold,
the correction disappears, i.e., it goes to 1, and
it is most dramatic for low $\theta$ at $k_i$ near the threshold.</p>
<p>Since we know the shape of the pdf for each $i$, we can multiply all pdf
to get the posterior (without normalization).</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>pdfs<span style="color:#f92672">=</span>[]
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> n_flip, k <span style="color:#f92672">in</span> zip(data[<span style="color:#e6db74">&#39;n&#39;</span>], data[<span style="color:#e6db74">&#39;k&#39;</span>]):
</span></span><span style="display:flex;"><span>    I <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span><span style="color:#f92672">-</span>stats<span style="color:#f92672">.</span>binom<span style="color:#f92672">.</span>cdf(k_t<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, n_flip, thetas)
</span></span><span style="display:flex;"><span>    pdfs<span style="color:#f92672">.</span>append(stats<span style="color:#f92672">.</span>binom<span style="color:#f92672">.</span>pmf(k, n_flip, thetas)<span style="color:#f92672">/</span>I)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>a<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>log(np<span style="color:#f92672">.</span>array(pdfs))<span style="color:#f92672">.</span>sum(axis<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>imax <span style="color:#f92672">=</span> a<span style="color:#f92672">.</span>argmax()
</span></span><span style="display:flex;"><span><span style="color:#75715e"># np.exp(-9)=0.0001234...; ignore ranges where pdf falls below this time the peak value</span>
</span></span><span style="display:flex;"><span>ii <span style="color:#f92672">=</span> a<span style="color:#f92672">-</span>a[imax]<span style="color:#f92672">&gt;-</span><span style="color:#ae81ff">9</span>   
</span></span><span style="display:flex;"><span>a<span style="color:#f92672">-=</span>a<span style="color:#f92672">.</span>max()   <span style="color:#75715e"># arbitrary scaling...</span>
</span></span><span style="display:flex;"><span>a<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>exp(a)
</span></span><span style="display:flex;"><span>a[<span style="color:#f92672">~</span>ii]<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>fig, ax <span style="color:#f92672">=</span>plt<span style="color:#f92672">.</span>subplots(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">4</span>,<span style="color:#ae81ff">3</span>))
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>plot(thetas, a<span style="color:#f92672">*</span><span style="color:#ae81ff">20</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">r</span><span style="color:#e6db74">&#39;$\propto \prod_i \, p(k_i,\,|\,n_i,\,\theta)$&#39;</span>)
</span></span><span style="display:flex;"><span>sns<span style="color:#f92672">.</span>kdeplot(r2[<span style="color:#e6db74">&#39;theta&#39;</span>], label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;from stan&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>axvline(theta)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>legend(fontsize<span style="color:#f92672">=</span><span style="color:#ae81ff">12</span>);
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">r</span><span style="color:#e6db74">&#39;$\theta$&#39;</span>);
</span></span></code></pre></div><p><img src="/files/coinflips/output_23_0.png" alt="png"></p>
<p>The two match as they should.</p>
<p>If there is an absolute threhold $k&gt;k_t$, this will affect experiments where you expect $k$ to be near $k_t$ whereas large-$n_i$ experiments will not be affected.</p>
<p>Let&rsquo;s carry out a comparison to explore this: collect 1000 experiments in total, but</p>
<ul>
<li>in case 1: throw 10 times in 100 experiments</li>
<li>in case 2: throw 100 times in 10 experiments</li>
</ul>
<p>and set the threshold at $k_t = 5$ with $\theta=0.2$ (Deliberately chosen to see the effect of data truncation).</p>
<p>Although the total number of flips is the same,
there is much higher chance that all experiments in case 1 will not be reported as we would typically expect $n_i \theta = 2$ heads
with some spread.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">simulate</span>(theta, N, n, k_t):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#39;&#39;&#39;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Throw n flips each time.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Threshold at k&gt;=k_t.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Fill until I have N records.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#39;&#39;&#39;</span>
</span></span><span style="display:flex;"><span>    n_i <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>ones(N, dtype<span style="color:#f92672">=</span>int)<span style="color:#f92672">*</span>n
</span></span><span style="display:flex;"><span>    k_i <span style="color:#f92672">=</span> stats<span style="color:#f92672">.</span>binom<span style="color:#f92672">.</span>rvs(n<span style="color:#f92672">=</span>n_i, p<span style="color:#f92672">=</span>theta)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    nbads <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># repeat until i have N experiments replacing truncated</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">while</span> (k_i<span style="color:#f92672">&lt;</span>k_t)<span style="color:#f92672">.</span>sum()<span style="color:#f92672">&gt;</span><span style="color:#ae81ff">0</span>:
</span></span><span style="display:flex;"><span>        bad <span style="color:#f92672">=</span> k_i <span style="color:#f92672">&lt;</span> k_t
</span></span><span style="display:flex;"><span>        nbads <span style="color:#f92672">+=</span> bad<span style="color:#f92672">.</span>sum()
</span></span><span style="display:flex;"><span>        k_i_new <span style="color:#f92672">=</span> stats<span style="color:#f92672">.</span>binom<span style="color:#f92672">.</span>rvs(n<span style="color:#f92672">=</span>n_i[bad], p<span style="color:#f92672">=</span>theta)
</span></span><span style="display:flex;"><span>        k_i[bad] <span style="color:#f92672">=</span> k_i_new
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">&#34;theta=</span><span style="color:#e6db74">{:.3f}</span><span style="color:#e6db74"> N=</span><span style="color:#e6db74">{:5d}</span><span style="color:#e6db74"> n=</span><span style="color:#e6db74">{:5d}</span><span style="color:#e6db74"> k_t=</span><span style="color:#e6db74">{:3d}</span><span style="color:#e6db74">: replaced </span><span style="color:#e6db74">{}</span><span style="color:#e6db74">&#34;</span><span style="color:#f92672">.</span>format(
</span></span><span style="display:flex;"><span>        theta, N, n, k_t, nbads))
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> dict(N<span style="color:#f92672">=</span>N, n<span style="color:#f92672">=</span>n_i, k<span style="color:#f92672">=</span>k_i,kthresh<span style="color:#f92672">=</span>k_t)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>theta<span style="color:#f92672">=</span><span style="color:#ae81ff">0.2</span>
</span></span><span style="display:flex;"><span>data1 <span style="color:#f92672">=</span> simulate(theta, <span style="color:#ae81ff">100</span>, <span style="color:#ae81ff">10</span>, <span style="color:#ae81ff">5</span>)
</span></span><span style="display:flex;"><span>data2 <span style="color:#f92672">=</span> simulate(theta, <span style="color:#ae81ff">10</span>, <span style="color:#ae81ff">100</span>, <span style="color:#ae81ff">5</span>)
</span></span><span style="display:flex;"><span>data3 <span style="color:#f92672">=</span> simulate(theta, <span style="color:#ae81ff">500</span>, <span style="color:#ae81ff">10</span>, <span style="color:#ae81ff">5</span>)
</span></span></code></pre></div><pre><code>theta=0.200 N=  100 n=   10 k_t=  5: replaced 3228
theta=0.200 N=   10 n=  100 k_t=  5: replaced 0
theta=0.200 N=  500 n=   10 k_t=  5: replaced 13591
</code></pre>
<p>Indeed, to collect 100, there were &gt;2500 experiments that were truncated because $k_i$ did not make the threshlold.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>result <span style="color:#f92672">=</span> {}
</span></span><span style="display:flex;"><span>result[<span style="color:#e6db74">&#39;$(N,\,n_i)$ = (100, 10)&#39;</span>] <span style="color:#f92672">=</span> modelth<span style="color:#f92672">.</span>sampling(data<span style="color:#f92672">=</span>data1)
</span></span><span style="display:flex;"><span>result[<span style="color:#e6db74">&#39;$(N,\,n_i)$ = (10, 100)&#39;</span>] <span style="color:#f92672">=</span> modelth<span style="color:#f92672">.</span>sampling(data<span style="color:#f92672">=</span>data2)
</span></span><span style="display:flex;"><span>result[<span style="color:#e6db74">&#39;$(N,\,n_i)$ = (500, 10)&#39;</span>] <span style="color:#f92672">=</span> modelth<span style="color:#f92672">.</span>sampling(data<span style="color:#f92672">=</span>data3)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>fig, ax <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplots(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">5</span>,<span style="color:#ae81ff">4</span>))
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> k, v <span style="color:#f92672">in</span> result<span style="color:#f92672">.</span>items():
</span></span><span style="display:flex;"><span>    sns<span style="color:#f92672">.</span>kdeplot(v[<span style="color:#e6db74">&#39;theta&#39;</span>], label<span style="color:#f92672">=</span>k, ax<span style="color:#f92672">=</span>ax)
</span></span><span style="display:flex;"><span>ax<span style="color:#f92672">.</span>axvline(theta, c<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;k&#39;</span>, lw<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>);
</span></span><span style="display:flex;"><span>ax<span style="color:#f92672">.</span>legend(loc<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;upper right&#39;</span>, fontsize<span style="color:#f92672">=</span><span style="color:#ae81ff">12</span>);
</span></span><span style="display:flex;"><span>ax<span style="color:#f92672">.</span>set(xlabel<span style="color:#f92672">=</span><span style="color:#e6db74">r</span><span style="color:#e6db74">&#39;$\theta$&#39;</span>);
</span></span></code></pre></div><p><img src="/files/coinflips/output_29_0.png" alt="png"></p>
<p>As expected, in this case it&rsquo;s better to have $(N, n_i)=(10, 100)$ than
$(N, n_i)=(100, 10)$ although the total number of flips is the same.
However, even in the case where each experiment is at the risk of being discarded due to
data truncation, if we have enough data and we account for the data truncation,
we can still recover more $\theta$ more precisely: we start to recover $\theta$ with
comparable confidence interval when 10-flip experiment records are collected 5 times more ($N=500$).</p>
<p>This cross-over between large-$N$-small-$n_i$ and small-$N$-large-$n_i$
depends on values of $\theta$ and $k_t$.</p>



<p>Tags:

	<a href='/tags/statistics'>Statistics</a></p>








	</main><footer>
    <p>© 2016-2022 Semyeong Oh / 
        Built with <a href="https://gohugo.io">Hugo</a> and <a href="https://simplecss.org/">Simple.css</a>.</p>
    <p>This website is hosted on <a href="http://github.com/smoh/smoh.github.io">Github</a>.</p>

</footer><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"></script>

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body);"></script>

<script>
    document.addEventListener("DOMContentLoaded", function () {
        renderMathInElement(document.body, {
            delimiters: [
                { left: "$$", right: "$$", display: true },
                { left: "$", right: "$", display: false }
            ]
        });
    });
</script>


</body>

</html>